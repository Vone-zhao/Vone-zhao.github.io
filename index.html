<!--<!doctype html>
<html>-->
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<head>
 <link rel="Shortcut Icon" href="./images/logo.png" sizes=16x16  type="image/x-icon" />
 <link rel="Bookmark" href="./images/logo.png" sizes=16x16 type="image/x-icon" />
<!--<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">-->
  <title>Bin Zhao</title>
	<style>
@media screen and (max-device-width: 480px){
  body{
    -webkit-text-size-adjust: none;
  }
}
p { font-size : 16px; }
h1 { font-size : 34px; margin : 0; padding : 0; }
h2 { font-size : 20px; margin : 0; padding : 0; }
h3 { font-size : 18px; margin : 8; padding : 0; }
body { padding : 0; font-family : Arial; font-size : 16px; background-color : #fff; }
.title { width : 650px; margin : 20px auto; }
.container { width : 700px; margin : 20px auto; border-radius: 10px;  background-color : #fff; padding : 20px;  clear:both;}
#bio {
    padding-top : 40px;
}
#me { border : 0 solid black; margin-bottom : 50px; border-radius : 10px; }
#sidebar { margin-left : 25px; border : 0 solid black; float : right; margin-bottom : 0;}
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #0050e7; }
.publogo { width: 100 px; margin-right : 10px; float : left; border : 0;}
.publication { display: flex; align-items: center; clear : left; padding-bottom : 0px; max-width: 900px;flex-wrap: wrap;}
.publication p { flex: 1; height : 100px; padding-top : 0px; font-weight: normal;}
.publication strong a { color : #0000A0; }
.publication .links { position :relative ; top : 10px }
.publication .links a { margin-right : 5px; font-size: 15px; font-weight: normal}
.codelogo { margin-right : 10px; float : left; border : 0;}
.code { clear : left; padding-bottom : 10px; vertical-align :middle;} 
.code .download a { display : block; margin : 0 15px; float : left;}
.code strong a { color : #000; }
.external a { margin : 0 10px; }
.external a.first { margin : 0 10px 0 0; }
li, ul {font-weight: normal;}
</style>
<link rel="stylesheet" href="stylesheets/styles.css">
<link rel="stylesheet" href="stylesheets/pygment_trac.css">
<meta name="viewport" content="width=device-width">
<script async="" src="./analytics.js"></script>
</head>
<body>
<div class="wrapper">

<header>
<h7>Bin Zhao</h7><br><br>
<div>
<img src="images/Bin_Zhao_Life.jpg" border="0" width="90%"><br></div><br>

  
<p>
<small>üìç Xi'an, Shannxi, China</small><br>
<small>üìßbinzhao111@gmail.com</small><br>
<small>üìßbin@nwpu.edu.cn</small><br>
	
<!--<a href="https://github.com/Li-Chongyi/" target="_blank">[GitHub]</a>-->  
<!--<a href="http://dblp.uni-trier.de/pers/hd/l/Li:Chongyi" target="_blank">[DBLP]</a>  <br>-->
<a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=DQB0hqwAAAAJ" target="_blank">[Google Scholar]</a> <br>
<a href="https://pjlab-ipec.com/" target="_blank">[IPEC Group]</a>
</p> <br>

<p class="view"><a href="https://github.com/Vone-zhao/">Github Homepage</a></p>
<!--<p class="view"><a href="sub_publication.html">Publications</a></p>-->
<!--<p class="view"><a href="datasets.html">Datasets</a></p>-->

	
<!--<p class="view"><a href="sub_projects.html">Projects</a></p>-->
</header>

<section>

<h2>
<a id="Biography-page" class="anchor" href="#biography-pages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Welcome to Bin Zhao (ËµµÊñå)'s Homepage</h2>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>
<br>I am a Associate Professor at Northwestern Polytechnical University, China. My research focuses on the integration of artificial intelligence with hardware and software, as well as embodied intelligence. Our team is dedicated to leveraging physical priors, multi-sensor integration, and mobile platforms to enhance the environmental perception, semantic interaction, and autonomous decision-making capabilities of intelligent agents such as humanoid robots, drones, robotic arms, and quadruped robots. We warmly welcome undergraduate students, as well as master's and doctoral students interested in computer vision, embodied intelligence, and robotic hardware, to join us for internships and academic exchanges.

<hr />
</p>




<!--
<h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Open Positions:</h2>
<br><p><font color="red">I am looking for several Postdoctoral Fellows at Nankai University (work in Tianjin or Shenzhen). The application deadline is 11 March 2024. If you wanna have a try, please drop me an email. </font></p></br>
</ul>
<br>	
<br><p><font color="blue">I am looking for PhD students and Master students who want to conduct
research and develop advanced deep learning algorithms for image and video enhancement and restoration, computational imaging, and image signal processor to join my research group at Nankai University (2023 Fall or 2024 Fall). I am also recruiting Research Associates and Final Year Project students.</font></p></br>
</ul>
<br>	
-->
<!--
<hr /> 
</p>
<h2>
<a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Research Interests:</h2>

<ul>
<div style="text-align: justify; display: block; margin-right: auto;">
<p>My primary reseach interests include artificial intelligence, machine learning, computer vision, and image processing, particularly in the domains of 
  <li><strong>Image and Video Restoration and Enhancement</strong></li> <br/>
	The purpose is to develop algorithms to process an image or video so that result is more suitable than original image or video for specific application. The specific research topics are 
  <ol type="a" start="1">
      <li>restoring and enhancing the images and videos captured in adverse weather (hazy, foggy, sandy, dusty, rainy, snowy day)</li>
      <li>restoring and enhancing the images and videos captured in special circumstances or devices (underwater, weak illumination, dark, under-display devices)</li>
      <li>general photo enhancement, auto image retouching</li>
      <li>image/depth super-resolution, image deblurring, image denosing</li>
  </ol>
  <li><strong>Multi-Modality Scene Understanding</strong> </li><br/>
	  The purpose is to design AI models to perceive and understand scenes. The specific research topics are
  <ol type="a" start="1">
      <li>RGB-D salient object detection</li>
      <li>co-salient object detection</li>
      <li>remote sensing image salient object detection</li>
  </ol>  

  
</ul>
<br>
<hr />
</p>
-->
<!--
<h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Recent News:</h2>
 
<ul>
<li> 2023/10 --3 papers (including one spotlight paper) got accepted by <strong>NeurIPS 2023</strong>.</li>
<li> 2023/09 --Two papers have been recognized as <strong>ESI Hot Paper</strong></li> 

</ul>
<br>
<hr />

-->

<div class="container">

<h2>
  <a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>News:</h2>
  <div id="news">
    <ul>
      <li><b>[12/2024]Two papers are accepted by AAAI2024</b></li>
      <li><b>[9/2024]One paper is accepted by CoRL2024</b></li>
      <li><b>[7/2024]One paper is accepted by ECCV2024</b></li>
      <li><b>[5/2024]One paper is accepted by RSS2024</b></li>
      <li><b>[5/2024]Three papers are accepted by ICML2024</b></li>
      <li><b>[4/2024]Two papers (EN-SLAM and GS-SLAM) are accepted as CVPR2024 Highlights</b></li>
      <li><b>[2/2024]Three papers are accepted by CVPR2024</b></li>
      <li><b>[1/2024]Two papers are accepted by ICRA2024</b></li>


    </ul>
    </div>
  <hr />

<h2>
  <a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Group:</h2>
  <table class="imgtable"><tbody>

    <tr>

    <td><div class="span3">
    <img src="./images/Li_Zhaojian.jpg" width="150" height="150" alt="" /> <br />
    <a href="https://dinglihe.github.io/" target="_blank"> Zhaojian Li ÊùéÊò≠ÂÅ• </a> <br /> Ph.D. candidate
    </div></td>
    
    <td><div class="span3">
    <img src="./images/Pengfei_Han.jpg" width="150" height="150" alt="" /> <br />
    <a href="https://zhiyuanyou.github.io/" target="_blank"> Pengfei Han Èü©ÈπèÈ£û </a><br />
    Ph.D. candidate <br />
    </div></td>

    <td><div class="span3">
      <img src="./images/Guanzhou_Lan.jpg" width="150" height="150" alt="" /> <br />
      <a href="https://yutian10.github.io/" target="_blank"> Guanzhou Lan ÂÖ∞ÂÜ†Ê¥≤ </a><br />
      Ph.D. candidate <br />
      </div></td>
      
    <td><div class="span3">
      <img src="./images/BinHao_Ren.jpg" width="150" height="150" alt="" /> <br />
      <a href="https://xiao-chen.tech/" target="_blank"> BinHao Ren ‰ªªÁÇ≥Êµ©</a> <br /> Ph.D. candidate
    </div></td>

    </tr>
    <tr>
    
    <td><div class="span3">
      <img src="./images/Kehui_Liu.jpg" width="150" height="150" alt="" /> <br />
      <a href="https://caixin98.github.io/" target="_blank"> Kehui Liu ÂàòÂù∑Âçâ</a> <br /> Ph.D. candidate
    </div></td>
          

    <td><div class="span3">
    <img src="./images/Dengdian_Huang.jpg" width="150" height="150" alt="" /> <br />
    <a href="https://" target="_blank"> Dengdian Huang ÈªÑÁôªÊÆø </a><br />
    Ph.D. candidate <br />
    </div></td>
    
    
    <td><div class="span3">
      <img src="./images/Yiwen_Tang.jpg" width="150" height="150" alt="" /> <br />
      <a href="https://gnwekge78707.github.io/" target="_blank"> Yiwen Tang Ê±§ËΩ∂Êñá </a><br />
      Master <br />
    </div></td>	

    <td><div class="span3">
      <img src="./images/Yunpeng_Gao.jpg" width="150" height="150" alt="" /> <br />
      <a href="https://gnwekge78707.github.io/" target="_blank"> Yunpeng Gao È´ò‰∫ëÈπè </a><br />
      Master <br />
      </div></td>	
    
    </tr>
    <tr>
      <td><div class="span3">
        <img src="./images/Junli_Liu.jpg" width="150" height="150" alt="" /> <br />
        <a href="https://" target="_blank"> Junli Liu Âàò‰øäÂà© </a><br />
        Master <br />
        </div></td>	
        
        <td><div class="span3">
        <img src="./images/Tang_Yuhang.jpg" width="150" height="150" alt="" /> <br />
        <a href="https://" target="_blank"> Yuhang Tang Ê±§ÂÆáËà™ </a><br />
        Master <br />
        </div></td>	

      <td><div class="span3">
        <img src="./images/YiXuan_Lou.jpg" width="150" height="150" alt="" /> <br />
        <a href="https://gnwekge78707.github.io/" target="_blank"> YiXuan Lou Ê•ºÈÄ∏ËΩ© </a><br />
        Master <br />
        </div></td>	

      <td><div class="span3">
          <img src="./images/JiaCheng_Bao.png" width="150" height="150" alt="" /> <br />
          <a href="https://gnwekge78707.github.io/" target="_blank"> JiaCheng Bao ÂåÖ‰Ω≥ËØö </a><br />
          Master <br />
        </div></td>	
    </tr>	

  </table>
  <!--  <div class="content group anchor" id="group">
    <div class="text front">
      <div class="title">Students</div> 
      <ul>
        <li><a href="https://samuelpclarke.com/">Dengdian Huang</a>(phD, with <a href="https://scholar.google.com/citations?hl=zh-CN&user=6FWhbDgAAAAJ">Yuan Yuan</a>)</li>
        <li><a href="https://ceyzaguirre4.github.io/">Kehui Liu</a> (phD, with <a href="https://scholar.google.com/citations?hl=zh-CN&user=ahUibskAAAAJ">Xuelong Li</a>)</li>
        <li><a href="https://chen-geng.com/">Guanzhou Lan</a>(phD, with <a href="https://scholar.google.com/citations?hl=zh-CN&user=ahUibskAAAAJ">Xuelong Li</a>)</li>
        <li><a href="https://samuelpclarke.com/">Binhao Ren</a>(phD, with <a href="https://scholar.google.com/citations?hl=zh-CN&user=6FWhbDgAAAAJ">Yuan Yuan</a>)</li>
        <li><a href="https://shellguo.com/">Pengfei Han</a> (phD, with <a href="https://scholar.google.com/citations?hl=zh-CN&user=ahUibskAAAAJ">Xuelong Li</a>, Recipient of the National Scholarship)</li>
        <li><a href="https://samuelpclarke.com/">Zhaojian Li</a>(phD, with <a href="https://scholar.google.com/citations?hl=zh-CN&user=6FWhbDgAAAAJ">Yuan Yuan</a>)</li>

        <li><a href="https://ericryanchan.github.io/">Junli Liu</a> (Master, with <a href="https://scholar.google.com/citations?hl=zh-CN&user=ahUibskAAAAJ">Xuelong Li</a>)</li>
        <li><a href="https://klemenkotar.github.io/">Yuhang Tang</a> (Master)</li>
        <li><a href="https://klemenkotar.github.io/">Yunpeng Gao</a> (Master)</li>
        <li><a href="https://klemenkotar.github.io/">Yiwen Tang</a> (Master, with <a href="https://scholar.google.com/citations?hl=zh-CN&user=ahUibskAAAAJ">Xuelong Li</a>, Recipient of the National Scholarship)</li>

      </ul>
      <div class="title">Master Student</div> 
      <ul>
        <li><a href="https://ericryanchan.github.io/">Junli Liu</a> (with <a href="https://stanford.edu/~gordonwz/">Xuelong Li</a>)</li>
        <li><a href="https://klemenkotar.github.io/">Yunpeng Gao</a> (with <a href="https://web.stanford.edu/~yamins">Dan Yamins</a>)</li>
        <li><a href="https://sunfanyun.com/">Fan-Yun Sun</a> (with <a href="https://www.autonomousagents.stanford.edu/">Nick Haber</a>)</li>
        <li><a href="https://maggiewang.org/">Maggie Wang</a> (with <a href="https://web.stanford.edu/~schwager/">Mac Schwager</a>)</li>
      </ul>
    -->
      <div class="title">Alumni</div> 
      <ul class="alumni">
        <li><a href="https://guanguanboy.github.io/">Guanlin Li</a> (phD 2024)</li>
        <li><a href="https://guanguanboy.github.io/">Fuhua Zhang</a> (Master 2023)</li>
        <li><a href="https://guanguanboy.github.io/">Xun Li</a> (Master 2023)</li>

      </ul>
    </div>
  </div>
<hr />

    
<h2><a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Selected Publications:</h2>
        <be>
	
	<div class="publication">
          <img src="images/publications/ARXIV-AlignBot.png" onmouseover="this.src='images/publications/ARXIV-AlignBot.png';" onmouseout="this.src='images/publications/ARXIV-AlignBot.png';" class="publogo"  width="300 px">
	<p>     
	
                <strong>
                    <a href="">AlignBot: Aligning VLM-powered Customized Task Planning with User Reminders Through Fine-Tuning for Household Robots</a>
                </strong>
		  <br> 
		<em><b>Arxiv, 2024 <a href="" target="_blank"><font color="#ff0000"></font></a></b></em>
                <br> 
                Zhaxizhuoma, Pengan Chen, Ziniu Wu, Jiawei Sun, Dong Wang, Peng Zhou, Nieqing Cao, Yan Ding, Bin Zhao, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2409.11905">PDF</a>| 
		    <a href="https://yding25.com/AlignBot/">Project Page</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br>
	<br />
	<br />
	<br />
	
	
	<div class="publication">
          <img src="images/publications/ARXIV-COHERENT.png" onmouseover="this.src='images/publications/ARXIV-COHERENT.png';" onmouseout="this.src='images/publications/ARXIV-COHERENT.png';" class="publogo"  width="300 px">
	<p>     
	
                <strong>
                    <a href="">COHERENT: Collaboration of Heterogeneous Multi-Robot System with Large Language Models</a>
                </strong>
		  <br> 
		<em><b>Arxiv, 2024 <a href="" target="_blank"><font color="#ff0000"></font></a></b></em>
                <br> 
                Kehui Liu, Zixin Tang2, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2409.15146">PDF</a>| 
		    <a href="https://github.com/SHAILAB-IPEC/COHERENT/">Project Page</a>| 	
                    <a href="https://github.com/MrKeee/COHERENT/">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br>
	<br />
	<br />
	<br />
	
	
	<div class="publication">
        <img src="images/publications/ARXIV-FASTUMI.png" onmouseover="this.src='images/publications/ARXIV-FASTUMI.png';" onmouseout="this.src='images/publications/ARXIV-FASTUMI.png';" class="publogo"  width="300 px"> 
	<p>     
	
                <strong>
                    <a href="">Fast-UMI: A Scalable and Hardware-Independent Universal Manipulation Interface</a>
                </strong>
		  <br> 
		<em><b>arxiv, 2024 <a href="" target="_blank"><font color="#ff0000"></font></a></b></em>
                <br> 
                Ziniu Wu, Tianyu Wang, Zhaxizhuoma, Chuyue Guan, Zhongjie Jia, Shuai Liang,
                Haoming Song, Delin Qu, Dong Wang, Zhigang Wang, Nieqing Cao,
                Yan Ding, Bin Zhao, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2409.19499">PDF</a>| 
		    <a href="https://fastumi.com/">Project Page</a>| 	
                    <a href="https://github.com/MrKeee/COHERENT/">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br>
	<br />
	<br />
	<br />

	<div class="publication">
      <img src="images/publications/ARXIV-VLN.png" onmouseover="this.src='images/publications/ARXIV-VLN.png';" onmouseout="this.src='images/publications/ARXIV-VLN.png';" class="publogo"  width="300 px">   
	<p>     
	
                <strong>
                    <a href="">Aerial Vision-and-Language Navigation via Semantic-Topo-Metric Representation Guided LLM Reasoning</a>
                </strong>
		  <br> 
		<em><b>Arxiv, 2024 </b></em>
                <br> 
                Yunpeng Gao, Zhigang Wang, Linglin Jing, Dong Wang, Xuelong Li, Bin Zhao.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2410.08500">PDF</a>| 
		    <a href="https://eziotic.github.io/STMR-VLN.github.io/">Project Page</a>| 	
                    <a href="https://github.com/sczhou/ProPainter">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br>
	<br />
	<br />
	<br />


	   <div class="publication">
            <img src="images/publications/CVPR2024-HPLESS.png" onmouseover="this.src='images/publications/CVPR2024-HPLESS.png';" onmouseout="this.src='images/publications/CVPR2024-HPLESS.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">HPL-ESS: Hybrid Pseudo-Labeling for Unsupervised Event-based Semantic Segmentation</a>
                </strong>
		  <br> 
		<em><b>CVPR, 2024 </b></em>
                <br> 
                Linglin Jing, Yiming Ding, Yunpeng Gao, Zhigang Wang, Xu Yan, Dong Wang1, Gerald Schaefer,Hui Fang, Bin Zhao, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Jing_HPL-ESS_Hybrid_Pseudo-Labeling_for_Unsupervised_Event-based_Semantic_Segmentation_CVPR_2024_paper.pdf">PDF</a>| 
		    <a href="https://github.com/SHAILAB-IPEC/HPL_ESS/">Project Page</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	 <be>
	 <br>
	<br />
	<br />
	<br />
	<br />


	
	   <div class="publication">
            <img src="images/publications/CORL2024-KOI.png" onmouseover="this.src='images/publications/CORL2024-KOI.png';" onmouseout="this.src='images/publications/CORL2024-KOI.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">KOI: Accelerating Online Imitation Learning via	Hybrid Key-state Guidance</a>
                </strong>
		  <br> 
		<em><b>CoRL, 2024 </b></em>
                <br> 
                Jingxian Lu, Wenke Xia, Dong Wang, Zhigang Wang, Bin Zhao, Di Hu, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2408.02912">PDF</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	 <br>
	<br />
	<br />
	<br />
	
	
	   <div class="publication">
            <img src="images/publications/ICML2024-SAME.png" onmouseover="this.src='images/publications/ICML2024-SAME.png';" onmouseout="this.src='images/publications/ICML2024-SAME.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">SAM-E: Leveraging Visual Foundation Model with Sequence Imitation for Embodied Manipulation</a>
                </strong>
		  <br> 
		<em><b>ICML, 2024 </b></em>
                <br> 
                Junjie Zhang, Chenjia Bai, Haoran He, Wenke Xia, Zhigang Wang, Bin Zhao, Xiu Li, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2405.19586">PDF</a>| 
		    <a href="https://sam-embodied.github.io/">Project Page</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
	<div class="publication">
          <img src="images/publications/ICML2024-CoRE.png" onmouseover="this.src='images/publications/ICML2024-CoRE.png';" onmouseout="this.src='images/publications/ICML2024-CoRE.png';" class="publogo"  width="300 px">
	<p>     
	
                <strong>
                    <a href="">Contrastive Representation for Data Filtering in Cross-Domain Offline Reinforcement Learning</a>
                </strong>
		  <br> 
		<em><b>ICML, 2024 </b></em>
                <br> 
                Xiaoyu Wen, Chenjia Bai, Kang Xu, Xudong Yu, Yang Zhang, Xuelong Li, Zhen Wang.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2405.06192">PDF</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br>
	<br />
	<br />
	<br />
	
	
	<div class="publication">
            <img src="images/publications/ICML2024-CeSD.png" onmouseover="this.src='images/publications/ICML2024-CeSD.png';" onmouseout="this.src='images/publications/ICML2024-CeSD.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Constrained Ensemble Exploration for Unsupervised Skill Discovery</a>
                </strong>
		  <br> 
		<em><b>ICML, 2024</b></em>
                <br> 
                Chenjia Bai, Rushuai Yang, Qiaosheng Zhang, Kang Xu, Yi Chen, Ting Xiao, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2405.16030">PDF</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br>
	<br />
	<br />
	<br />
	
		<div class="publication">
            <img src="images/publications/CVPR2024-ENSLAM.png" onmouseover="this.src='images/publications/CVPR2024-ENSLAM.png';" onmouseout="this.src='images/publications/CVPR2024-ENSLAM.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Implicit Event-RGBD Neural SLAM</a>
                </strong>
		  <br> 
		<em><b>CVPR, 2024 Highlight</b></em>
                <br> 
                Delin Qu, Chi Yan, Dong Wang, Jie Yin, Dan Xu, Bin Zhao, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2311.11013">PDF</a>| 
		    <a href="https://delinqu.github.io/EN-SLAM/">Project Page</a>| 	
                    <a href="https://github.com/DelinQu/EN-SLAM/">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	 <br>
	<br />
	<br />
	<br />
	
		  <div class="publication">
            <img src="images/publications/CVPR2024-GSSLAM.png" onmouseover="this.src='images/publications/CVPR2024-GSSLAM.png';" onmouseout="this.src='images/publications/CVPR2024-GSSLAM.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting</a>
                </strong>
		  <br> 
		<em><b>CVPR, 2024 </b></em>
                <br> 
                Chi Yan, Delin Qu, Dan Xu, Bin Zhao,
                Zhigang Wang, Dong Wang, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2311.11700.pdf">PDF</a>| 
		    <a href="https://gs-slam.github.io/">Project Page</a>| 	
                    <a href="https://gs-slam.github.io/">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br>
	<br />
	<br />
	<br />
	

      <div class="publication">
            <img src="images/publications/AAAI2024-x4d.png" onmouseover="this.src='images/publications/AAAI2024-x4d.png';" onmouseout="this.src='images/publications/AAAI2024-x4d.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">X4D-SceneFormer:  Enhanced Scene Understanding on 4D Point Cloud Videos through Cross-modal  Knowledge Transfer</a>
                </strong>
		  <br> 
		<em><b>AAAI, 2024 </b></em>
                <br> 
                Linglin Jing, Ying Xue, Xu Yan, Chaoda Zheng, Dong Wang, Ruimao Zhang, Zhigang Wang, Hui Fang, Bin Zhao, Zhen Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2312.07378">PDF</a>| 	
                    <a href="https://github.com/jinglinglingling/X4D">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
	  <div class="publication">
            <img src="images/publications/AAAI2024-POINT.png" onmouseover="this.src='images/publications/AAAI2024-POINT.png';" onmouseout="this.src='images/publications/AAAI2024-POINT.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models</a>
                </strong>
		  <br> 
		<em><b>AAAI, 2024</b></em>
                <br> 
                Yiwen Tang, Ray Zhang, Zoey Guo, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2310.03059">PDF</a>| 
		    <a href="https://github.com/Ivan-Tang-3D/Point-PEFT?tab=readme-ov-file">Project Page</a>| 	
                    <a href="https://github.com/Ivan-Tang-3D/Point-PEFT">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br>
	<br />
	<br />
	<br />


<!--	
	
	<div class="publication">
 <img src="logo/Beauty_REC_dataset.png" onmouseover="this.src='logo/Beauty_REC_dataset.png';" onmouseout="this.src='logo/Beauty_REC_dataset.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">BeautyREC: Robust, Efficient, and Content-preserving Makeup Transfer</a>
                </strong>
		  <br> 
		<em><b>NTIRE CVPRW, 2023 <a href="" target="_blank"><font color="#ff0000">[Oral]</font></a></b></em>
                <br> 
              Qixin Yan, Chunle Guo, Jixin Zhao, Yuekun Dai, Chen Change Loy, and <b>Chongyi Li<sup>+</sup></b>.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2212.05855">PDF</a>| 
		    <a href="https://li-chongyi.github.io/BeautyREC_files/">Project Page</a>| 	
                    <a href="https://github.com/learningyan/BeautyREC/">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />

		<div class="publication">
 <img src="logo/FlexiCurve.png" onmouseover="this.src='logo/FlexiCurve.png';" onmouseout="this.src='logo/FlexiCurve.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">FlexiCurve: Flexible Piecewise Curves Estimation for Photo Retouching</a>
                </strong>
		  <br> 
		<em><b>NTIRE CVPRW, 2023 <a href="" target="_blank"><font color="#ff0000">[Oral]</font></a></b></em>
                <br> 
              <b>Chongyi Li</b>, Chunle Guo, Shangchen Zhou, Qiming Ai, Ruicheng Feng, and Chen Change Loy.
                <br>
                <span class="links">
                    <a href="https://openaccess.thecvf.com/content/CVPR2023W/NTIRE/papers/Li_FlexiCurve_Flexible_Piecewise_Curves_Estimation_for_Photo_Retouching_CVPRW_2023_paper.pdf">PDF</a>| 
		    <a href="https://li-chongyi.github.io/FlexiCurve">Project Page</a>| 	
                    <a href="https://li-chongyi.github.io/FlexiCurve">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	 <br>
	 <br>
	<br />
	<br />
	<br />
	<br />
		
-->	
	
	  <div class="publication">
            <img src="images/publications/AAA12024-COLOR.png" onmouseover="this.src='images/publications/AAA12024-COLOR.png';" onmouseout="this.src='images/publications/AAA12024-COLOR.png';" class="publogo"  width="300 px">

	<p> 
                <strong>
                    <a href="">Color  Event Enhanced Single-Exposure HDR Imaging</a>
                </strong>
		  <br> 
		<em><b>AAAI, 2024</b></em>
                <br> 
                Mengyao Cui, Zhigang Wang, Dong Wang, Bin Zhao, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27904">PDF</a>
                </span>
            </p>
          </div>
          <br>
	<br>
       <br />
	<br />
	
	<div class="publication">
            <img src="images/publications/OE-ASF.png" onmouseover="this.src='images/publications/OE-ASF.png';" onmouseout="this.src='images/publications/OE-ASF.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">ASF-Transformer</a>
                </strong>
		  <br> 
		<em><b>Optics Express, 2024 </b></em>
                <br> 
                Ziran Zhang, Bin Zhao, Yueting Chen, Zhigang Wang, Dong Wang, Jiawei Sun, Jie Zhang, Zhihai Xu, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://pubmed.ncbi.nlm.nih.gov/38017848/">PDF</a>| 
		    <a href="https://li-chongyi.github.io/FourierUp_files/">Project Page</a>| 	
                    <a href="https://github.com/manman1995/Deep-Fourier-Upsampling">Code</a>
                </span>
            </p>
          </div>
          <br>
	<br />

	  <div class="publication">
            <img src="images/publications/NC.png" onmouseover="this.src='images/publications/NC.png';" onmouseout="this.src='images/publications/NC.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">AI-driven projection tomography with multicore fibre-optic cell rotation</a>
                </strong>
		  <br> 
		<em><b>Nature Communications, 2024</b></em>
                <br> 
                Jiawei Sun, Bin Yang, Nektarios Koukourakis, Jochen Guck, Juergen W. Czarske.
                <br>
                <span class="links">
                    <a href="https://www.nature.com/articles/s41467-023-44280-1">PDF</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	 <br>
	<br />
	<br />
	<br />
	

	


	 <div class="publication">
            <img src="images/publications/OL-CAL.png" onmouseover="this.src='images/publications/OL-CAL.png';" onmouseout="this.src='images/publications/OL-CAL.png';" class="publogo"  width="300 px">
       
	<p> 
                <strong>
                    <a href="">Calibration-free  quantitative phase imaging in multi-core fiber endoscopes using end-to-end deep learning</a>
                </strong>
		  <br> 
		<em><b>Optics Letters, 2024</b></em>
                <br> 
                Jiawei Sun, Bin Zhao, Dong Wang, Zhigang Wang, Jie Zhang, Nektarios Koukourakis, J√∫ergen W. Czarske, and Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://opg.optica.org/ol/abstract.cfm?uri=ol-49-2-342">PDF</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	



	 <div class="publication">
          <img src="images/publications/ECCV2024-ANY2POINT.png" onmouseover="this.src='images/publications/ECCV2024-ANY2POINT.png';" onmouseout="this.src='images/publications/ECCV2024-ANY2POINT.png';" class="publogo"  width="300 px">
      
            <p> 
                <strong>
                    <a href="">Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding</a>
                </strong>
		  <br> 
		<em><b>ECCV, 2024 </b></em>
                <br> 
                Yiwen Tang, Ray Zhang, Jiaming Liu, Zoey Guo, Dong Wang, Zhigang Wang, Bin Zhao, Shanghang Zhang, Peng Gao, Hongsheng Li, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2404.07989">PDF</a>| 
		    <a href="https://github.com/Ivan-Tang-3D/Any2Point">Project Page</a>| 	
                    <a href="https://github.com/Ivan-Tang-3D/Any2Point">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />


	 <div class="publication">
          <img src="images/publications/ARXIV-LARGE.png" onmouseover="this.src='images/publications/ARXIV-LARGE.png';" onmouseout="this.src='images/publications/ARXIV-LARGE.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning</a>
                </strong>
		  <br> 
		<em><b>Arxiv, 2024</b></em>
                <br> 
                Haoran He, Chenjia Bai, Ling Pan, Weinan Zhang, Bin Zhao, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/html/2402.14407v1">PDF</a>| 
		    <a href="https://video-diff.github.io/">Project Page</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />

<div class="publication">
           <img src="images/publications/ICRA2024-KINEMATIC.png" onmouseover="this.src='images/publications/ICRA2024-KINEMATIC.png';" onmouseout="this.src='images/publications/ICRA2024-KINEMATIC.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Kinematic-aware Prompting for Generalizable Articulated Object Manipulation with LLMs</a>
                </strong>
		  <br> 
		<em><b>ICRA, 2024</b></em>
                <br> 
                Wenke Xia, Dong Wang, Xincheng Pang, Zhigang Wang, Bin Zhao, Di Hu, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2311.02847">PDF</a>| 
		    <a href="https://github.com/GeWu-Lab/LLM_articulated_object_manipulation">Project Page</a>| 	
                    <a href="https://github.com/GeWu-Lab/LLM_articulated_object_manipulation">Code</a>	
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
		<!-- 
<div class="publication">
           <img src="logo/TCYB2022 _input.jpg" onmouseover="this.src='logo/TCYB2022_output.jpg';" onmouseout="this.src='logo/TCYB2022 _input.jpg';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Global-and-Local Collaborative Learning for Co-Salient Object Detection</a>
                </strong>
		  <br> 
		<em><b>TCYB, 2022</b></em>
                <br> 
              Runmin Cong, Ning Yang, Chongyi Li, Huazhu Fu, Yao Zhao, Qingming Huang, and Sam Kwong.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2204.08917.pdf">PDF</a>| 
		    <a href="https://rmcong.github.io/proj_GLNet.html/">Project Page</a>| 	
                    <a href="https://github.com/rmcong/GLNet_TCYB2022">Code</a>	
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
	-->


		 
	<div class="publication">
            <img src="images/publications/ICRA2024-ROBUST.png" onmouseover="this.src='images/publications/ICRA2024-ROBUST.png';" onmouseout="this.src='images/publications/ICRA2024-ROBUST.png';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Robust quadrupedal locomotion via risk-averse policy learning</a>
                </strong>
		 <br> 
		<em><b>ICRA, 2024</b></em>
                <br> 
                Jiyuan Shi, Chenjia Bai, Haoran He, Lei Han, Dong Wang2, Bin Zhao,
                Mingguo Zhao, Xiu Li, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2308.09405">PDF</a>| 
                    <a href="https://risk-averse-locomotion.github.io/">Project Page</a>| 
                    <a href="https://risk-averse-locomotion.github.io/">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
		 
	  <div class="publication">
            <img src="images/publications/NIPS2023-DIFFUSION.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning</a>
                </strong>
		  <br> 
		<em><b>NeurIPS , 2023 </b></em>
                <br> 
                Haoran He, Chenjia Bai, Kang Xu, Zhuoran Yang, Weinan Zhang,
                Dong Wang, Bin Zhao, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/ccda3c632cc8590ee60ca5ba226a4c30-Paper-Conference.pdf">PDF</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	

	
	  <div class="publication">
            <img src="images/publications/NIPS2023-CROSS.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a>Cross-Domain Policy Adaptation via Value-Guided	Data Filtering</a>
                </strong>
		  <br>
		<em><b>NIPS, 2023 </b></em>
                <br>
                Kang Xu1, Chenjia Bai, Xiaoteng Ma, Dong Wang, Bin Zhao, Zhen Wang, Xuelong Li, Wei Li.
                <br>
                <span class="links">
                    <a href="https://proceedings.neurips.cc/paper_files/paper/2023/file/e8ad87f1076fb0f75d89a45828f186b0-Paper-Conference.pdf">PDF</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	


	
        <div class="publication">
            <img src="images/publications/ARXIV-MOTION.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">Motion-Aware Video Frame Interpolation</a>
                </strong>
		  <br>
		<em><b>arXiv , 2023</b></em>
                <br>
                Pengfei Han, Fuhua Zhang, Bin Zhao, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2402.02892">PDF</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
        <div class="publication">
            <img src="images/publications/AI-PESS.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">Pessimistic Value Iteration for Multi-Task Data Sharing in Offline Reinforcement Learning</a>
                </strong>
		   <br>
		<em><b>Artificial Intelligenc, 2023</b></em>
                <br>
                Chenjia Bai, Lingxiao Wang, Jianye Hao, Zhuoran Yang, Bin Zhaoa, Zhen Wange, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2404.19346">PDF</a>| 
                    <a href="https://github.com/Baichenjia/UTDS">Project Page</a>| 
                    <a href="https://github.com/Baichenjia/UTDS">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	

	<!--
        <div class="publication">
            <img src="./logo/tgars_logo.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org/document/8793227">Nested Network With Two-Stream Pyramid for Salient Object Detection in Optical Remote Sensing Images</a>
                </strong>
		     <br>
		 <em><b>TGRS, 2020 <a href="" target="_blank"><font color="#ff0000">[ESI Highly Cited Paper]</font></a></b></em>
                <br>
                <b>Chongyi Li</b>, Runmin Cong, Junhui Hou, Sanyi Zhang, Yue Qian, Sam Kwong.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org/document/8793227">PDF</a>| 
                    <a href="https://li-chongyi.github.io/proj_optical_saliency.html">Project Page</a>| 
		    <a href="https://drive.google.com/file/d/15FOnRo1xnz05fcNkXBhWy8WL0C26i8y4/view">Benchmark</a>| 
                    <a href="https://drive.google.com/file/d/1nnZKphu9_4oBvie4yqdxG95tYZRsqj4W/view">Results</a>
                </span>
           </p>
        </div>
        <br>
        <br>
	<br />
	<br />
	-->
        <div class="publication">
            <img src="images/publications/TPAMI-VEHICLE.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">Vehicle Perception from Satellite</a>
                </strong>
		     <br>
		 <em><b>IEEE transactions on pattern analysis and machine intelligence, 2023 </b></em>
                <br>
                Bin Zhao, Pengfei Han, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2402.00703">PDF</a>| 
                    <a href="https://github.com/Chenxi1510/Vehicle-Perception-from-Satellite-Videos">Project Page</a>| 
                    <a href="https://github.com/Chenxi1510/Vehicle-Perception-from-Satellite-Videos">Code</a>
                </span>
           </p>
        </div>
        <br>
	<br>
	<br />
	<br />
	<!--
        <div class="publication">
            <img src="./logo/TC2020_logo.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="https://ieeexplore.ieee.org.remotexs.ntu.edu.sg/stamp/stamp.jsp?tp=&arnumber=8998588">ASIF-Net: Attention Steered Interweave Fusion Network for RGB-D Salient Object Detection</a>
                </strong>
		     <br>
		<em><b>TCYB, 2020 <a href="" target="_blank"><font color="#ff0000">[ESI Highly Cited Paper & TCYB Popular Articles]</font></a></b></em>
                <br>
                <b>Chongyi Li</b>, Runmin Cong, Sam Kwong, et al.
                <br>
                <span class="links">
                    <a href="https://ieeexplore.ieee.org.remotexs.ntu.edu.sg/stamp/stamp.jsp?tp=&arnumber=8998588">PDF</a>| 
                    <a href="https://github.com/Li-Chongyi/ASIF-Net">Code and Results</a>
                </span>
           </p>
        </div>
        <br>
	<br>
	<br />
	<br />
	-->


	
        <div class="publication">
            <img src="images/publications/ICCV2023-VIEWREFER.png" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance</a>
                </strong>
		     <br>
		<em><b>ICCV, 2023</b></em>
                <br>
                Zoey Guo, Yiwen Tang, Ray Zhang, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/pdf/2303.16894.pdf">PDF</a>| 
                    <a href="https://github.com/Ivan-Tang-3D/ViewRefer3D">Project Page</a>| 
                    <a href="https://github.com/Ivan-Tang-3D/ViewRefer3D">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
	 <br />
	 <br />	

	
        <div class="publication">
            <img src="images/publications/ICCV2023-ROLLING.png" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Towards Nonlinear-Motion-Aware and Occlusion-Robust	Rolling Shutter Correction</a>
                </strong>
		     <br>
		 <em><b>ICCV, 2023 </b></em>
                <br>
                Delin Qu, Yizhen Lao, Zhigang Wang, Dong Wang, Bin Zhao, Xuelong Li.
                <br>
                <span class="links">
                    <a href="https://arxiv.org/abs/2303.18125">PDF</a>| 
                    <a href="">Project Page</a>| 
                    <a href="https://delinqu.github.io/QRSC/">Code</a>
                </span>
            </p>
          </div>
          <br>
          <br>
</div>
<br>
<hr />





<!--
<h2>
<a id="new-page" class="anchor" href="#new-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Events:</h2>  

<div class="publication">
           <img src="./logo/MIPI_2023.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">Mobile Intelligent Photography & Imaging</a>
                </strong>
		  <br> 
		<em><b>2nd MIPI workshop @ CVPR 2023</b></em>
                <br> 
              <b>Chongyi Li</b>, Shangchen Zhou, Ruicheng Feng,  Yuekun Dai, Qingpeng Zhu, Qianhui Sun,  Wenxiu Sun,  Chen Change Loy, and Jinwei Gu.
                <br>
                <span class="links">
		      <a href="https://mipi-challenge.org/MIPI2023/">Website</a>	
		      
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />

<div class="publication">
            <img src="logo/codeformer1.jpg" onmouseover="this.src='logo/codeformer2.jpg';" onmouseout="this.src='logo/codeformer1.jpg';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Try Your Face (face restoration platform)</a>
                </strong> 
              We provide our code of CodeFormer (Towards Robust Blind Face Restoration with Codebook Lookup TransFormer) on Colab. Please feel free to try your face.
                <br>
                <span class="links">
		      <a href="https://colab.research.google.com/drive/1m52PNveE4PBhYrecj34cnpEeiHcC5LTb?usp=sharing">Colab</a>	
		      
                </span>
            </p>
          </div>
          <br>
	  <br>
	<br />
	<br />
	<br />
	
<div class="publication">
            <img src="logo/plateform1.jpg" onmouseover="this.src='logo/plateform2.jpg';" onmouseout="this.src='logo/plateform1.jpg';" class="publogo"  width="300 px">
            <p> 
                <strong>
                    <a href="">Low-Light Image Enhancement Online Platform</a>
                </strong> 
              Different algorithms demand various configurations, GPU versions, and hardware specifications that are prohibitive to beginners who are new to this area and may not even have GPU resources. We contribute an online plateform.
                <br>
                <span class="links">
		      <a href="http://mc.nankai.edu.cn/ll/">Website</a>	
		      
                </span>
            </p>
          </div>
          <br>
	  <br>
	<br />
	<br />
	<br />
	
	  <div class="publication">
           <img src="./logo/MIPI.png" class="publogo" width="300 px">
            <p> 
                <strong>
                    <a href="">Mobile Intelligent Photography & Imaging</a>
                </strong>
		  <br> 
		<em><b>1st MIPI workshop @ ECCV 2022</b></em>
                <br> 
              <b>Chongyi Li</b>, Shangchen Zhou, Ruicheng Feng, Jun Jiang, Wenxiu Sun, Qingyu Yang, Qingpeng Zhu, Chen Change Loy, and Jinwei Gu.
                <br>
                <span class="links">
		      <a href="https://mipi-challenge.org/MIPI2022/">Website</a>	
		      
                </span>
            </p>
          </div>
          <br>
          <br>
	<br />
	<br />
	
<ul>
<li><a href="http://mipi-challenge.org/" target="_blank"><font color="#A52A2A">[Workshop]</font></a> ECCV 2022 Workshop on Mobile Intelligent Photography and Imaging (MIPI)</a></li>
<li><a href="https://attend.ieee.org/mmsp-2022/special-sessions/underwater-multimedia-processing/" target="_blank"><font color="#A52A2A">[Special Session]</font></a> IEEE MMSP 2022 Special Session on Underwater Multimedia Processing</a></li>
<li><a href="https://www.frontiersin.org/research-topics/39049/multimodal-intelligence" target="_blank"><font color="#A52A2A">[Special Issue]</font></a> Frontiers in Signal Processing Special Issue on Multimodal Intelligence</a></li>
<li><a href="https://ieeeoes.org/wp-content/uploads/2021/07/JOE_cfp_AMLM.pdf" target="_blank"><font color="#A52A2A">[Special Issue]</font></a> IEEE Journal of Oceanic Engineering Special Issue on Advanced Machine Learning Methodologies for Underwater Image and Video Processing and Analysis (2021-2022)</a></li>
<li><a href="https://github.com/Li-Chongyi/PAPERS/blob/master/MTAP_SI_CFP.pdf" target="_blank"><font color="#A52A2A">[Special Issue]</font></a> Multimedia Tools and Applications Special Issue on Depth-Related Processing and Applications in Visual Systems (2020-2021)</a></li>
<li><a href="https://github.com/Li-Chongyi/PAPERS/blob/master/SPIC_SI_cfp.pdf" target="_blank"><font color="#A52A2A">[Special Issue]</font></a> Signal Processing: Image Communication Special Issue on Visual Information Processing for Underwater Images and Videos: Theories, Algorithms, and Applications (2019-2020)</a></li>
<li><a href="https://github.com/Li-Chongyi/PAPERS/blob/master/APSIPA-ASC-2019-CfP.pdf" target="_blank"><font color="#A52A2A">[Special Session]</font></a> APSIPA ASC 2019 Special Session on Multi-source Data Processing and Analysis: Models, Methods and Applications (2019-2020)</a></li>

</ul>
<br>
<hr />


		 


		 
<h2>
<a id="reaserch-page" class="anchor" href="#reaserch-page" aria-hidden="true"><span class="octicon octicon-link"></span></a>Miscellaneous:</h2>

 
<ul>
<li><a href="https://unsplash.com/"><font color="#1C86EE">Unsplash</font></a></li>
<li><a href="https://pngtree.com/"><font color="#1C86EE">Pngtree</font></a></li>
<li><a href="https://www.wordclouds.com/"><font color="#1C86EE">WordClouds</font></a></li>
<li><a href="https://emojipedia.org/"><font color="#1C86EE">Emojipedia</font></a></li>
<li><a href="https://film-grab.com/"><font color="#1C86EE">FilmGrab</font></a></li>
<li><a href="https://deviparikh.medium.com/how-we-write-rebuttals-dc84742fece1/"><font color="#1C86EE">How we write rebuttals</font></a></li>
<li><a href="http://www-net.cs.umass.edu/kurose/writing/intro-style.html"><font color="#1C86EE">Writing a good introduction</font></a></li>
<li><a href="https://www.computer.org/publications/tech-news/trends/deep-learning-vs-machine-learning-whats-the-difference?source=cssocial"><font color="#1C86EE">Deep Learning vs Machine Learning: What‚Äôs the Difference</font></a></li>
</ul>
<br>
-->	

<!--<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=a&t=tt&d=pPHWAkKgmzsFC_v7-3ndOuL5q3qL_EhEE16zTJwxtRw"></script> -->
<!--<div align="center"><a href="http://www.amazingcounters.com"><img border="0" src="http://cc.amazingcounters.com/counter.php?i=3244445&c=9733648" alt="AmazingCounters.com"></a></div>--> 
<!--div align="center"><a href="http://www.amazingcounters.com"><img border="0" src="http://cc.amazingcounters.com/counter.php?i=3230662&c=9692299" alt="AmazingCounters.com"></a></div>-->
<!-- Global site tag (gtag.js) - Google Analytics -->
<!--<script async src="https://www.googletagmanager.com/gtag/js?id=UA-156698907-1"></script>-->




</section>

</div>
<!--<script src="javascripts/scale.fix.js"></script>-->
</body>
</html>  
